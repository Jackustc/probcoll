# contains all needed parameters for all files

dt: 0.5
random_seed: 0

exp_dir: '/home/bding/Desktop/probcoll/experiments/bebop2d'
exp_name: test
#run: prediction # gps, dagger, replay, prediction

##########################
### Probcoll algorithm ###
##########################

probcoll:
  T: 10 # timesteps per trajectory
  max_iter: 10
  planner_type: 'primitives' # primitives / cem / ilqr / randomwalk / randomcontrolset

  dyn_noise: [0.01, 0.01, 0.] # how much noise added in dynamics
  control_noise:
    type: 'gaussian' # zero / gaussian / OU / smoothedgaussian

    zero: {}
    gaussian:
      std: [0.1, 0.1, 0]
    smoothedgaussian:
      std: [0.1, 0.1, 0]
      T: 30
    ou:
      std: [0.1, 0.1, 0]
      mean: [0.25, 0.0, 0.0]
      theta: 0.15


  label_with_noise: True # if false, saves desired controls (i.e. without dyn_noise)
  obs_noise: True # boolean use noise or not

#  init_data: '/home/gkahn/code/gps_quadrotor/experiments/pointquad/data'

  use_ground_truth: False
  use_init_cost: True # TODO
  epsilon_greedy: 0

  cost:
    weight: 1e5
    eval_cost: 'speed * speed * sigmoid(probs_mean + 0.0*probs_std)'
    pre_activation: True

  conditions:
    repeats: 20 # TODO
    num_test: 1
    randomize_conds: False
    randomize_reps: True

    default:
      linearvel: [0., 0, 0.0]

    range:
      linearvel:
        min: [0, 0, 0]
        max: [0, 0, 0]
        num: [1, 1, 1]

    perturb: # repetition perturbations
      linearvel: [0.0, 0.0, 0.0]




#######################################################
################ objective + optimizer ################
#######################################################

trajopt:
  cost_velocity:
    velocity: [0.8, 0.]
    weights: [1., 0.2]

mpc:
  H: 4

  ilqr:
    warm_start: True
    use_threading: False
    max_iter: 2

  cem:
    init:
      init_var: 5.0
      init_M: 200
      K: 20
      M: 50
      iters: 10
    warm_start:
      init_var: 0.1
      init_M: 200
      K: 20
      M: 50
      iters: 3

##########################################
########## Training environment ##########
##########################################

world:
  randomize: True

##################################
########### Prediction ###########
##################################

model:
  T: 4 # how many timesteps to predict

  graph_type: 'fc' # fc / cnn / rnn
  num_bootstrap: 5 # 5
  dropout: 0.95 # pct keep (None if no dropout)
  reg: 0.

  # what to use as input to prediction model
  X_order: []
  U_order: ['linearvel']
  O_order: ['camera']
  output_order: ['collision'] # taken from observations O
  use_O_orth: False

  # NN training parameters
  device: 1
  gpu_fraction: 0.4
  reset_every_train: True # every time train is called, reinitialize weights?
  early_stopping: False # if True, only saves model each epoch if it's the lowest cost on validation for all epochs
  learning_rate: 0.001
  batch_size: 16
  display_batch: 5
  epochs: 100
  steps: 800
  val_pct: 0.2

  # how to resample data
  aggregate_save_data: True # put all data into one file?
  save_type: 'fixedlen' # varlen / fixedlen
  balance:
    type: collision # none, collision, uncertainty

    collision:
      pct_coll: 0.5

  # neural network parameters
  hidden_layer: 25
  activation: relu


#################################################
### Planning objective and optimizer settings ###
#################################################

planning:
  cost_velocity:
    velocity: [0.5, 0., 0.]
    weights: [1., 0.0, 0.]

#prediction:
#
#  dagger:
#    T: 10 # timesteps per trajectory
#    max_iter: 10
#    planner_type: 'primitives' # primitives / teleop
#
#    control_noise:
#      type: 'zero' # zero / gaussian / OU / smoothedgaussian
#
#      zero: {}
#      gaussian:
#        std: [0.1, 0.1]
#
#    label_with_noise: True # if false, saves desired controls (i.e. without control_noise)
#
##    init_data: '/home/gkahn/code/gps_quadrotor/experiments/bebop2d/lfd_cmd_vel_init'
#    init_epochs: 100
#
#    use_init_cost: True # TODO
#    epsilon_greedy: 0
#
#    cost_probcoll:
#      weight: 1e1 # TODO
##      eval_cost: 'speed * speed * probs_mean + speed * speed * probs_std'
##      pre_activation: False
#
#      eval_cost: 'speed * speed * sigmoid(probs_mean + 0.0*probs_std)'
#      pre_activation: True

#    conditions:
#      repeats: 20 # TODO
#      num_test: 1
#      randomize_conds: False
#      randomize_reps: True
#
#      default:
#        linearvel: [0., 0]
#
#      range:
#        linearvel:
#          min: [0, 0]
#          max: [0, 0]
#          num: [1, 1]
#
#      perturb: # repetition perturbations
#        linearvel: [0.0, 0.0]

bebop:
  topics:
    image: '/bebop/image_raw' # sensor_msgs/Image
    cmd_vel: '/vservo/cmd_vel' # geometry_msgs/Twist
    measured_vel: '/bebop/states/ardrone3/PilotingState/SpeedChanged' # bebop_msgs/Ardrone3PilotingStateSpeedChanged
    cmd_acc: '/bebop/cmd_vel' # geometry_msgs/Twist

    collision: '/bebop/collision' # std_msgs/Empty
    start_rollout: '/bebop/start_rollout' # std_msgs/Empty
    bad_rollout: '/bebop/bad_rollout' # std_msgs/Empty

    debug_image: '/bebop/debug/image' # sensor_msgs/Image
    debug_cmd_vel: '/bebop/debug/cmd_vel' # visualization_msgs/Marker


######################################################
########### States, controls, observations ###########
######################################################

X:
  dim: 3
  order: ['linearvel']

  linearvel: {idx: 0, dim: 3}
#  dim: 13
#  order: ['position', 'orientation', 'linearvel', 'angularvel']
#
#  position:    {idx: 0,  dim: 3}
#  orientation: {idx: 3,  dim: 4} # wxyz
#  linearvel:   {idx: 7,  dim: 3}
#  angularvel:  {idx: 10, dim: 3}

U:
  dim: 3
  order: ['linearvel']

  linearvel: {idx: 0, dim: 3}

O:
  dim: 257
  order: ['camera', 'collision']

  camera: # TODO
    idx: 0
    dim: 256
    height: 16
    width: 16
    noise: 0.01
  collision: {idx: 256, dim: 1, buffer: 0.0}

###################
####### LQR #######
###################

ilqr:
  # regularization for positive definite Quu
  mu_start: 1.0 # 1.0
  mu_mult: 1.6
  mu_min: 0.1
  mu_max: 10 # 10 # Fu is very small, requires large mu to make Quu PD
  dmu_start: 1.0
  reg_state: True # True # new regularization scheme
  reg_control: False # False # old one, Quu + mu * I

  # line search for policy
  alpha_start: 1.0
  alpha_mult: 0.1
  alpha_min: 0.00000001

  # misc
  max_exit_mu: .3
  min_cost_delta: 0.005 # 0.05
  max_iter: 20
  z_min: 0.

  plot: False

###################
####### CEM #######
###################

cem:
  init_var: 0.05
  init_M: 40 # 40
  K: 10 # 5
  M: 50 # 20
  iters: 10 # 5
